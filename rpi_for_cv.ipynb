{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(text,src):\n",
    "    cv.imshow(text,src)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackImages(scale,imgArray):\n",
    "    rows = len(imgArray)\n",
    "    cols = len(imgArray[0])\n",
    "    rowsAvailable = isinstance(imgArray[0], list)\n",
    "    width = imgArray[0][0].shape[1]\n",
    "    height = imgArray[0][0].shape[0]\n",
    "    if rowsAvailable:\n",
    "        for x in range ( 0, rows):\n",
    "            for y in range(0, cols):\n",
    "                if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]:\n",
    "                    imgArray[x][y] = cv.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x][y] = cv.resize(imgArray[x][y], (imgArray[0][0].shape[1],\n",
    "                                                                imgArray[0][0].shape[0]),None, scale, scale)\n",
    "                if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv.cvtColor( imgArray[x][y], cv.COLOR_GRAY2BGR)\n",
    "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "        hor = [imageBlank]*rows\n",
    "        hor_con = [imageBlank]*rows\n",
    "        for x in range(0, rows):\n",
    "            hor[x] = np.hstack(imgArray[x])\n",
    "        ver = np.vstack(hor)\n",
    "    else:\n",
    "        for x in range(0, rows):\n",
    "            if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                imgArray[x] = cv.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "            else:\n",
    "                imgArray[x] = cv.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale)\n",
    "            if len(imgArray[x].shape) == 2: imgArray[x] = cv.cvtColor(imgArray[x], cv.COLOR_GRAY2BGR)\n",
    "        hor= np.hstack(imgArray)\n",
    "        ver = hor\n",
    "    return ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize and Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv.imread('my_image/me.jpg')\n",
    "img_resized=cv.resize(img,(300,200))\n",
    "show_img('Fixed Resizing',img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized_aspect_ratio=imutils.resize(img,width=300)\n",
    "show_img('Aspect Ratio Resize',img_resized_aspect_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rotated_pos=imutils.rotate(img,45)\n",
    "img_rotated_neg=imutils.rotate(img,-45)\n",
    "img_rotated = stackImages(0.5,([img,img_rotated_pos],[img,img_rotated_neg]))\n",
    "\n",
    "show_img('Rotation',img_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rotated_bound=imutils.rotate_bound(img,45)\n",
    "show_img('Rotation Bound',img_rotated_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "3\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "**** 17 objects found **** \n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_shapes=cv.imread('chapter04-opencv_tutorial\\images\\shapes.png')\n",
    "img_copy=img_shapes.copy()\n",
    "img_gray=cv.cvtColor(img_shapes,cv.COLOR_BGR2GRAY)\n",
    "img_blur=cv.GaussianBlur(img_gray,(3,3),0)\n",
    "img_edged=cv.Canny(img_blur,50,150)\n",
    "\n",
    "\n",
    "cnts=cv.findContours(img_edged,cv.RETR_EXTERNAL,cv.CHAIN_APPROX_SIMPLE)\n",
    "# list of contours \n",
    "cnts=imutils.grab_contours(cnts)\n",
    "\n",
    "total=0\n",
    "for cnt in cnts:\n",
    "    # ignore small areas (noises)\n",
    "    if cv.contourArea(cnt) <25:\n",
    "        continue\n",
    "    cv.drawContours(img_copy,[cnt],-1,(204,0,255),2)\n",
    "    total+=1\n",
    "    \n",
    "    peri=cv.arcLength(cnt,True) #curve length \n",
    "    # print(peri)\n",
    "    approx=cv.approxPolyDP(cnt,epsilon=0.02*peri,closed=True)\n",
    "    print(len(approx)) #3=> triangle #4 =>rectangle or Square #4> => curve\n",
    "\n",
    "    obj_cor=len(approx)\n",
    "    \n",
    "    x,y,w,h=cv.boundingRect(approx)\n",
    "    \n",
    "    obj_type=''\n",
    "    if obj_cor==3:\n",
    "        obj_type='Triangle'\n",
    "    elif obj_cor==5:\n",
    "        obj_type='Pentagon'\n",
    "    elif obj_cor==6:\n",
    "        obj_type='WTFFF'\n",
    "    elif obj_cor==4:\n",
    "        asp_ratio=w/float(h)\n",
    "        if asp_ratio >0.95 and asp_ratio < 1.05 :\n",
    "            obj_type='Square'\n",
    "        else:\n",
    "            obj_type='Rectangle'\n",
    "    else:\n",
    "        obj_type='Curve'\n",
    "    \n",
    "    cv.rectangle(img_copy,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv.putText(img_copy,obj_type,\n",
    "                (x+(w//2)-20,y+(h//2)),cv.FONT_HERSHEY_COMPLEX,0.6,\n",
    "                (56, 32, 24),2)   \n",
    "    \n",
    "print(f'**** {total} objects found **** ')\n",
    "\n",
    "img_stack = stackImages(0.5,([img_shapes,img_gray,img_copy],[img_blur,img_edged,img_copy]))\n",
    "show_img('Counting Objects',img_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vars :** The method returns the __dict__ attribute for a module, class, instance, or any other object if the same has a __dict__ attribute. If the object fails to match the attribute, it raises a TypeError exception. Objects such as modules and instances have an updatable __dict__ attribute however, other objects may have written restrictions on their __dict__ attributes. vars() acts like locals() method when an empty argument is passed which implies that the locals dictionary is only useful for reads since updates to the locals dictionary are ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# # construct the argument parser and parse the arguments\n",
    "\n",
    "# ap=argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-b','--bg', required=True,help='path to the background image')\n",
    "# ap.add_argument('-f','--fg', required=True,help='path to the foreground image')\n",
    "\n",
    "# args=vars(ap.parse_args())\n",
    "\n",
    "# img_bg=cv.imread(args['bg'])\n",
    "# img_fg=cv.imread(args['fg'])\n",
    "\n",
    "# # execute this script \n",
    "\n",
    "# python [NAME].py --bg [BG_PATH] --fg [FG_PATH]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "img_bg=cv.imread(\"my_image\\me_bg.jpg\")\n",
    "img_fg=cv.imread(\"my_image\\me_fg.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg : [71 72 73 74 74 74 75 76 75 75 74 73 73] ,\n",
      "fg : [43 43 42 43 44 46 47 47 48 47 48 49 48] ,\n",
      "sub : [28 29 31 31 30 28 28 29 27 28 26 24 25] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# show images\n",
    "# show_img('img_bg',img_bg)\n",
    "# show_img('img_fg',img_fg)\n",
    "\n",
    "# convert background and foreground images to grayscale\n",
    "img_bg_gray=cv.cvtColor(img_bg,cv.COLOR_BGR2GRAY)\n",
    "img_fg_gray=cv.cvtColor(img_fg,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# perform background subtraction | background image (int32) - foreground image (int32) |\n",
    "img_sub=img_bg_gray.astype('int32')-img_fg_gray.astype('int32')\n",
    "img_sub=np.absolute(img_sub).astype('uint8')\n",
    "\n",
    "print(f'bg : {img_bg_gray[0][:13]} ,\\nfg : {img_fg_gray[0][:13]} ,\\nsub : {img_sub[0][:13]} ')\n",
    "\n",
    "show_img('img_sub',img_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basics of Erosion:** \n",
    " \n",
    "Erodes away the boundaries of the foreground object <br>\n",
    "Used to diminish the features of an image. <br> <br>\n",
    "\n",
    "\n",
    "**Basics of dilation:**\n",
    " \n",
    "\n",
    "Increases the object area <br>\n",
    "Used to accentuate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thresh=cv.threshold(img_sub,0,255,cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n",
    "img_erode=cv.erode(img_thresh,None,iterations=1)\n",
    "img_dilate=cv.dilate(img_erode,None,iterations=1)\n",
    "\n",
    "# # show images\n",
    "# img_subtraction = stackImages(0.7,([img_bg_gray,img_fg_gray,img_sub],[img_thresh,img_erode,img_dilate]))\n",
    "# show_img('img_subtraction',img_subtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contour detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours in the thresholded difference map and then initialize our bounding box regions\n",
    "# that contains the *entire* region of motion. \n",
    "\n",
    "cnts=cv.findContours(img_thresh.copy(),cv.RETR_EXTERNAL,cv.CHAIN_APPROX_SIMPLE)\n",
    "cnts=imutils.grab_contours(cnts)\n",
    "\n",
    "(min_x,min_y)=(np.inf,np.inf)\n",
    "(max_x,max_y)=(-np.inf,-np.inf)\n",
    "\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "    # compute the bounding box of the contour\n",
    "    (x,y,w,h)=cv.boundingRect(c)\n",
    "    \n",
    "    \n",
    "    # reduse noises by enforcing  requirements on the bounding box size \n",
    "    if w>20 and h>20:\n",
    "        # update our bookkeeping variables\n",
    "        min_x=min(min_x,x)\n",
    "        min_y=min(min_y,y)\n",
    "        max_x=max(max_x,x+w-1)\n",
    "        max_y=max(max_y,y+h-1)\n",
    "        \n",
    "# draw a rectangle surrounding the region of motion\n",
    "cv.rectangle(img_fg,(min_x,min_y), (max_x,max_y),(0,255,0) ,3)\n",
    "\n",
    "# show images\n",
    "img_subtraction = stackImages(0.3,([img_bg_gray,img_fg_gray,img_sub],[img_thresh,img_dilate,img_fg]))\n",
    "show_img('img_subtraction',img_subtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Object and Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load images\n",
    "img_family=cv.imread('my_image\\\\family.jpg')\n",
    "\n",
    "# resize image\n",
    "img_family=imutils.resize(img_family,width=700)\n",
    "\n",
    "# convert image to grayscale\n",
    "img_family_gray=cv.cvtColor(img_family,cv.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**detectMultiScale :**\n",
    "\n",
    "**scaleFactor** – Parameter specifying how much the image size is reduced at each image scale.<br>\n",
    "1.05 is a good possible value for this, which means you use a small step for resizing, i.e. reduce the size by 5%, you increase the chance of a matching size with the model for detection is found.<br><br>\n",
    "\n",
    "\n",
    "**minNeighbors** – Parameter specifying how many neighbors each candidate rectangle should have to retain it. <br>\n",
    "This parameter will affect the quality of the detected faces. Higher value results in fewer detections but with higher quality. 3~6 is a good value for it.<br><br>\n",
    "\n",
    "**minSize** – Minimum possible object size. Objects smaller than that are ignored.<br>\n",
    "This parameter determines how small size you want to detect. You decide it! Usually, [30, 30] is a good start for face detection.<br><br>\n",
    "\n",
    "**maxSize** – Maximum possible object size. Objects bigger than this are ignored.\n",
    "This parameter determines how big size you want to detect. Again, you decide it! Usually, you don't need to set it manually, the default value assumes you want to detect without an upper limit on the size of the face.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] detected 6\n"
     ]
    }
   ],
   "source": [
    "# load face detector \n",
    "detector=cv.CascadeClassifier('haarcascade_frontalface_alt_tree.xml')\n",
    "\n",
    "# detect faces in the image\n",
    "rects=detector.detectMultiScale(img_family_gray,scaleFactor=1.05,minNeighbors=3,minSize=(20,20),flags=cv.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "# total number of faces in image\n",
    "print(f'[INFO] detected {len(rects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the bounding boxes and draw a rectangle around each face\n",
    "for (x,y,w,h) in rects:\n",
    "\n",
    "    cv.rectangle(img_family,(x,y), (x+w,y+h),(0,255,0) ,2)\n",
    "\n",
    "show_img('img family',img_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[INFO] starting video stream ')\n",
    "# initialize the video stream\n",
    "vs = VideoStream(src=-1).start()\n",
    "# vs=VideoStream(src=0,usePiCamera=True,resolution=(640,480))\n",
    "\n",
    "# start thewebcam video stream and turn off the autofoucos setting\n",
    "vs.stream.set(cv.CAP_PROP_AUTOFOCUS,0)\n",
    "\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    frame = imutils.resize(frame,width=400)\n",
    "    \n",
    "    # show the output frame\n",
    "    cv.imshow('Frame',frame)\n",
    "    key= cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the 'q' ker was pressed, break from the loop\n",
    "    if key== ord('q'):\n",
    "        break\n",
    "\n",
    "# do a bit for cleanup\n",
    "cv.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 7\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[http @ 0x10a3d40] Stream ends prematurely at 1219647591, should be 18446744073709551615\n"
     ]
    }
   ],
   "source": [
    "def toggle_autofocus(vs,autofocus=True):\n",
    "    # set the autofocus camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_AUTOFOCUS,1 if autofocus else 0)\n",
    "    print(f'[INFO] autofocus has been set to {\"ON\" if autofocus else \"OFF\"}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualAutofocus=vs.stream.get(cv.CAP_PROP_AUTOFOCUS)\n",
    "    print(f'[INFO] actual autofocus {actualAutofocus}')\n",
    "\n",
    "\n",
    "def toggle_auto_whitebalance(vs,autowb=True):\n",
    "    # set the auto whitebalance camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_AUTO_WB,1 if autowb else 0)\n",
    "    print(f'[INFO] auto white balance has been set to {\"ON\" if autowb else \"OFF\"}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualAutoWB=vs.stream.get(cv.CAP_PROP_AUTO_WB)\n",
    "    print(f'[INFO] actual auto white balance {actualAutoWB}')\n",
    "\n",
    "def set_zoom(vs,zoom=100):\n",
    "    # set the zoom camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_ZOOM,zoom)\n",
    "    print(f'[INFO] zoom has been set to {zoom}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualZoom=vs.stream.get(cv.CAP_PROP_ZOOM)\n",
    "    print(f'[INFO] actual zoom {actualZoom}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream \n",
      "[INFO] auto white balance has been set to OFF\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] auto white balance has been set to ON\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] auto white balance has been set to OFF\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] auto white balance has been set to ON\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] auto white balance has been set to OFF\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] auto white balance has been set to ON\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] zoom has been set to 101\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 102\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 103\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 102\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 101\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 100\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] zoom has been set to 99\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] auto white balance has been set to OFF\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] zoom has been set to 98\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] auto white balance has been set to ON\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] zoom has been set to 97\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] auto white balance has been set to OFF\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] zoom has been set to 96\n",
      "[INFO] actual zoom 0.0\n",
      "[INFO] autofocus has been set to ON\n",
      "[INFO] actual autofocus 0.0\n",
      "[INFO] auto white balance has been set to ON\n",
      "[INFO] actual auto white balance 0.0\n",
      "[INFO] zoom has been set to 100\n",
      "[INFO] actual zoom 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1ae9900] overread 3\n",
      "[mjpeg @ 0x1ae9900] overread 3\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n",
      "[mjpeg @ 0x1ae9900] overread 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[INFO] starting video stream ')\n",
    "# initialize the video stream\n",
    "url='http://192.168.0.190:8080/video'\n",
    "vs = VideoStream(src=url).start()\n",
    "\n",
    "# initialize the camera parameter settings\n",
    "autofocus=True\n",
    "autowb=True\n",
    "zoom=100\n",
    "# vs.stream\n",
    "# vs.set(cv.CAP_PROP_ZOOM,50)\n",
    "# time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    frame = imutils.resize(frame,width=600)\n",
    "    \n",
    "    # show the output frame\n",
    "    cv.imshow('Frame',frame)\n",
    "    key= cv.waitKey(1)\n",
    "    \n",
    "    # handle *q* keypresses for \"quit\"\n",
    "\n",
    "    if key== ord('q'):\n",
    "        break  \n",
    "\n",
    "    # handle *f* keypresses for \"autofocus\"\n",
    "    elif key== ord('f'):\n",
    "        # toggle autofocus and set the camera property\n",
    "        autofocus= not autofocus\n",
    "        toggle_autofocus(vs,autofocus)\n",
    "\n",
    "    # handle *w* keypresses for \"auto white balance\"\n",
    "    elif key== ord('w'):\n",
    "        # toggle auto white balance and set the camera property\n",
    "        autowb= not autowb\n",
    "        toggle_auto_whitebalance(vs,autowb)\n",
    "\n",
    "    # handle *i* keypresses for \"zoom in\"\n",
    "    elif key== ord('i'):\n",
    "        # increase zoom  and set the camera property\n",
    "        zoom+=1\n",
    "        set_zoom(vs,zoom)\n",
    "\n",
    "    # handle *o* keypresses for \"zoom out\"\n",
    "    elif key== ord('o'):\n",
    "        # decrease zoom  and set the camera property\n",
    "        zoom-=1\n",
    "        set_zoom(vs,zoom)\n",
    "\n",
    "# reset camera parameter settings\n",
    "toggle_autofocus(vs,True)\n",
    "toggle_auto_whitebalance(vs,True)\n",
    "set_zoom(vs,100)\n",
    "\n",
    "# do a bit for cleanup\n",
    "vs.stop()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Lapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Capture Time Lapse Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import signal\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] warming up camera ...\n",
      "[INFO] press `ctrl + c` to exit, or \"q\" to quit if you have the display option on ...\n",
      "[INFO] cleaning up ...\n"
     ]
    }
   ],
   "source": [
    "def signal_handler(sig,frame):\n",
    "    print('[INFO] You pressed `ctrl + c`! Your pictures are saved' \\\n",
    "          ' in the output directory you specified :) ')\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "# ap=argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-o','--output',required=True,\n",
    "#     help='Path to the output directory')\n",
    "# ap.add_argument('-d','--delay',type=float,default=5.0,\n",
    "#     help='Delay in seconds between frames captured')\n",
    "# ap.add_argument('-dp','--display',type=int,default=0,\n",
    "#     help='Boolean used to indicate if frames should be displayed')\n",
    "\n",
    "# arg=vars(ap.parse_args())\n",
    "\n",
    "OUTPUT_PATH='my_image/output_dir'\n",
    "DELAY=2\n",
    "DISPLAY=True\n",
    "\n",
    "\n",
    "# initialize the output directory path and create the output directory \n",
    "output_dir=os.path.join(OUTPUT_PATH,\n",
    "        datetime.now().strftime('%Y-%m-%d-%H%M'))\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "# initialize the video stream \n",
    "print('[INFO] warming up camera ...')\n",
    "\n",
    "url='http://192.168.0.116:8080/video'\n",
    "vs=VideoStream(src=url,resolution=(1920,1280), framerate=30).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# set the frame count to zero \n",
    "count=0\n",
    "\n",
    "# signal trap to handle keyboard interrupt \n",
    "signal.signal(signal.SIGINT,signal_handler)\n",
    "print('[INFO] press `ctrl + c` to exit, or \"q\" to quit if you have the display option on ...')\n",
    "\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    \n",
    "    # draw the timestamp on the frame\n",
    "    ts=datetime.now().strftime('%A %d %B %Y %I:%M:%S%p')\n",
    "    cv.putText(frame,ts,(10,frame.shape[0]-10),cv.FONT_HERSHEY_SIMPLEX,0.35,(0,0,255),1)\n",
    "\n",
    "    # write the current frame to output directory\n",
    "    filename=f'{str(count).zfill(16)}.jpg' \n",
    "    cv.imwrite(os.path.join(output_dir,filename),frame)\n",
    "\n",
    "    # display the frame and detect keypressess if the flag is set\n",
    "    if DISPLAY:\n",
    "        # show the output frame\n",
    "        cv.imshow('Frame',frame)\n",
    "        key= cv.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the 'q' ker was pressed, break from the loop\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # increment the count\n",
    "        count+=1\n",
    "\n",
    "        # sleep for specified number of seconds\n",
    "        time.sleep(DELAY)\n",
    "\n",
    "# do a bit for cleanup\n",
    "print('[INFO] cleaning up ...')\n",
    "if DISPLAY:\n",
    "    cv.destroyAllWindows()\n",
    "vs.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing Time Lapse Images into a Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Using legacy 'setup.py install' for progressbar, since package 'wheel' is not installed.\n",
      "Installing collected packages: progressbar\n",
      "    Running setup.py install for progressbar ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import paths\n",
    "import progressbar\n",
    "import signal\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('my_image/output_dir/2022-05-07-1256/0000000000000000.jpg'.split(os.path.sep)[-1][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building my_image/video_dir/2022-05-07-1256.avi ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cleaning up ...\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH='my_image/output_dir/2022-05-07-1256/'\n",
    "OUTPUT_PATH='my_image/video_dir'\n",
    "# Frames per secend\n",
    "FPS=15\n",
    "\n",
    "# function to get the frame number from the image path\n",
    "def get_number(img_paths):\n",
    "    return int(img_paths.split(os.path.sep)[-1][:-4])\n",
    "\n",
    "\n",
    "\n",
    "# initialize the FourCC and video writer\n",
    "fourcc=cv.VideoWriter_fourcc(*'MJPG')\n",
    "writer=None\n",
    "\n",
    "# grab the paths to the images, and initialize output file name and output path\n",
    "img_paths=list(paths.list_images(INPUT_PATH))\n",
    "output_file=f'{INPUT_PATH.split(os.path.sep)[2]}.avi'\n",
    "output_path=os.path.join(OUTPUT_PATH,output_file)\n",
    "\n",
    "print(f'[INFO] building {output_path} ...')\n",
    "\n",
    "\n",
    "# initialize the progress bar\n",
    "widgets=['Building Video : ',progressbar.Percentage(),' ',progressbar.Bar(),' ', progressbar.ETA()]\n",
    "pbar=progressbar.ProgressBar(maxval=len(img_paths),widgets=widgets).start()\n",
    "\n",
    "# loop over all stored image paths\n",
    "for (i,img_path) in enumerate(sorted(img_paths,key=get_number)):\n",
    "    # print('helooo')\n",
    "    # load the image\n",
    "    img=cv.imread(img_path)\n",
    "\n",
    "    # initialize the video writer if needed\n",
    "    if writer is None:\n",
    "        (H,W) = img.shape[:2]\n",
    "        writer=cv.VideoWriter(output_path,fourcc,FPS,(W,H),True)\n",
    "    \n",
    "    # write the image to output video\n",
    "    writer.write(img)\n",
    "    pbar.update(i)\n",
    "\n",
    "\n",
    "# release the writer object \n",
    "print('[INFO] cleaning up ...')\n",
    "pbar.finish()\n",
    "writer.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bird Feeder Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting json_minify\n",
      "  Downloading JSON_minify-0.3.0-py2.py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: json-minify\n",
      "Successfully installed json-minify-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install json_minify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JSON-minify : \n",
    "`JSON-minify` minifies blocks of JSON-like content into valid JSON by removing\n",
    "all whitespace *and* JS-style comments (single-line `//` and multi-line\n",
    "`/* .. */`).\n",
    "\n",
    "With `JSON-minify`, you can maintain developer-friendly JSON documents, but\n",
    "minify them before parsing or transmitting them over-the-wire.\n",
    "\n",
    "###### json.loads() : \n",
    "json. loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'picamera': False,\n",
       " 'bg_sub': 'GMG',\n",
       " 'erode': {'kernel': [3, 3], 'iterations': 2},\n",
       " 'dilate': {'kernel': [5, 5], 'iterations': 3},\n",
       " 'min_radius': 80,\n",
       " 'keyclipwriter_buffersize': 50,\n",
       " 'codec': 'MJPG',\n",
       " 'write_snaps': True,\n",
       " 'frames_between_snaps': 30,\n",
       " 'annotate': True,\n",
       " 'display': True,\n",
       " 'output_path': 'Resources/src/bird feeder monitor/output_gmg',\n",
       " 'fps': 20}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confPath='Resources/src/bird feeder monitor/config/gmg.json'\n",
    "json.loads(json_minify(open(confPath).read()))\n",
    "# json_minify(open(confPath).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.config import Conf\n",
    "from Resources.src.utils.clipwriter import KeyClipWriter\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import datetime \n",
    "import imutils\n",
    "import time \n",
    "import sys\n",
    "import cv2 as cv\n",
    "import os\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the JSON configuration file \n",
    "CONF_PATH='Resources/src/bird_feeder_monitor/config/gmg.json'\n",
    "# path to the optional input video file \n",
    "VIDEO_PATH='Resources/src/bird_feeder_monitor/birds_10min.mp4'\n",
    "# VIDEO_PATH=''\n",
    "\n",
    "# STREAM_URL='http://192.168.0.190:8080/video'\n",
    "STREAM_URL=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.9/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.14.5 in ./venv/lib/python3.9/site-packages (from opencv-contrib-python) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] opening video file Resources/src/bird_feeder_monitor/birds_10min.mp4\n",
      "[INFO] detecting motion and storing videos and images ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mohammad/Desktop/Mohammad/Computer Vision/Projects/notebook/New Folder/RPI-master/rpi_for_cv.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=39'>40</a>\u001b[0m \u001b[39m# loop over the frames \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=40'>41</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=41'>42</a>\u001b[0m     \u001b[39m# grab a frame from the video stream\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=42'>43</a>\u001b[0m     full_frame\u001b[39m=\u001b[39mvs\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=44'>45</a>\u001b[0m     \u001b[39m# if no frames was read, the stream has ended\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mohammad/Desktop/Mohammad/Computer%20Vision/Projects/notebook/New%20Folder/RPI-master/rpi_for_cv.ipynb#ch0000046?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m full_frame \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# load our configuration settings\n",
    "conf=Conf(CONF_PATH)\n",
    "# conf.__getitem__('fps')\n",
    "\n",
    "# check if we are using a camera and start video stream \n",
    "if not VIDEO_PATH:\n",
    "    vs=VideoStream(src=STREAM_URL,resolution=(1920,1280), framerate=30,usePiCamera=conf['picamera']).start()\n",
    "    time.sleep(3.0)\n",
    "else:\n",
    "    print(f'[INFO] opening video file {VIDEO_PATH}')\n",
    "    vs=cv.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# OpenCV background subtraction\n",
    "OPENCV_BG_SUBTRACTIONS={\n",
    "    'CNT': cv.bgsegm.createBackgroundSubtractorCNT(),\n",
    "    'GMG': cv.bgsegm.createBackgroundSubtractorGMG(),\n",
    "    'MOG': cv.bgsegm.createBackgroundSubtractorMOG(),\n",
    "    'GSOC': cv.bgsegm.createBackgroundSubtractorGSOC(),\n",
    "    'LSBP': cv.bgsegm.createBackgroundSubtractorLSBP()\n",
    "}\n",
    "\n",
    "# create our background substractor\n",
    "fgbg=OPENCV_BG_SUBTRACTIONS[conf['bg_sub']]\n",
    "\n",
    "# create erosion and dilation kernels\n",
    "eKernel=np.ones(tuple(conf['erode']['kernel']),'uint8')\n",
    "dKernel=np.ones(tuple(conf['dilate']['kernel']),'uint8')\n",
    "\n",
    "# initialize key clip writer, the consecutive number of frames without \n",
    "# motion and frame since the last snapshot was written\n",
    "kcw=KeyClipWriter(bufSize=conf['keyclipwriter_buffersize']) \n",
    "frames_without_motion=0\n",
    "frames_since_snap=0\n",
    "\n",
    "# begin captureing \"ctrl+c\" signals\n",
    "# signal.signal(signal.SIGINT,signal_handler)\n",
    "images=' and images ...' if conf['write_snaps'] else '...'\n",
    "print(f'[INFO] detecting motion and storing videos{images}') \n",
    "\n",
    "# loop over the frames \n",
    "while True:\n",
    "    # grab a frame from the video stream\n",
    "    full_frame=vs.read()\n",
    "    \n",
    "    # if no frames was read, the stream has ended\n",
    "    if full_frame is None:\n",
    "        break\n",
    "    \n",
    "    # handle the frame whether the frame was read from a VideoCapture\n",
    "    # or VideoStream\n",
    "    full_frame=full_frame[1] if VIDEO_PATH else full_frame\n",
    "    \n",
    "    # increment number of frames since last snapshot was written\n",
    "    frames_since_snap+=1\n",
    "\n",
    "    # resize the frame apply the background subtractor to generate motion mask\n",
    "    frame=imutils.resize(full_frame,width=500)\n",
    "    mask=fgbg.apply(frame)\n",
    "    \n",
    "    # perform erosions and dilations to eliminate noise and fill gaps\n",
    "    mask=cv.erode(mask,eKernel,iterations=conf['erode']['iterations'])\n",
    "    mask=cv.dilate(mask,dKernel,iterations=conf['dilate']['iterations'])\n",
    "\n",
    "    # find contours in the mask and reset the motion status\n",
    "    cnts=cv.findContours(mask.copy(),cv.RETR_EXTERNAL,\n",
    "                         cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnts=imutils.grab_contours(cnts)\n",
    "    motion_this_frame=False\n",
    "    \n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # It is a circle which completely covers the object with minimum area\n",
    "        ((x,y),radius)=cv.minEnclosingCircle(c)\n",
    "        (rx,ry,rw,rh)=cv.boundingRect(c)\n",
    "        \n",
    "        # convert floating point values to integers\n",
    "        (x,y,radius)=[int(v) for v in (x,y,radius)]        \n",
    "        \n",
    "        # only process motion contours above the specified size\n",
    "        if radius < conf['min_radius']:\n",
    "            continue\n",
    "        # grab the current timestamp \n",
    "        timestamp=datetime.datetime.now()\n",
    "        timestring=timestamp.strftime('%Y%m%d-%H%M%S')\n",
    "        \n",
    "        # set our motion flag to indicate we have found motion and\n",
    "        # reset the motion counter\n",
    "        motion_this_frame=True\n",
    "        frames_without_motion=0\n",
    "        \n",
    "        # check if we need to annotate the frame for display\n",
    "        if conf['annotate']:\n",
    "            cv.circle(frame,(x,y),radius,(0,0,255),2)\n",
    "            cv.rectangle(frame,(rx,ry),(rx+rw,ry+rh),(0,255,0),2)\n",
    "        \n",
    "        # frame to disk\n",
    "        write_frame=frames_since_snap >= conf['frames_between_snaps']\n",
    "        \n",
    "        # check to see if should write the frame to disk\n",
    "        if conf['write_snaps'] and write_frame:\n",
    "            # construct the path to output photo and save it\n",
    "            snap_path=os.path.sep.join([conf['output_path'],timestring])\n",
    "            cv.imwrite(snap_path+'.jpg',full_frame)\n",
    "            \n",
    "            # reset the counter between snapshots\n",
    "            frames_since_snap=0\n",
    "        \n",
    "        # start recording if we aren't already\n",
    "        if not kcw.recording:\n",
    "            # construct the path to the video file\n",
    "            video_path=os.path.sep.join([conf['output_path'],timestring])\n",
    "\n",
    "            # instantiate the video codec object and start the key clip writer\n",
    "            fourcc=cv.VideoWriter_fourcc(*conf['codec'])\n",
    "            kcw.start('{}.avi'.format(video_path),fourcc,conf['fps']) \n",
    "        \n",
    "        # check if no motion was detected in this frame and then increment \n",
    "        # the number of consecutive frames without motion\n",
    "        if not motion_this_frame:\n",
    "            frames_without_motion+=1\n",
    "            \n",
    "        # update the key clip buffer\n",
    "        kcw.update(frame)\n",
    "    \n",
    "        # check to see if the number of frames without motion is above our defined threshold\n",
    "        no_motion=frames_without_motion >= conf['keyclipwriter_buffersize']\n",
    "        \n",
    "        # stop recording if there is no motion\n",
    "        if kcw.recording and no_motion:\n",
    "            kcw.finish()\n",
    "        \n",
    "        # check to see if we'er displaying the frame to our screen\n",
    "        if conf['display']:\n",
    "            # display the frame and grab keypresses\n",
    "            cv.imshow('Frames',frame) \n",
    "            key=cv.waitKey(1) & 0xFF\n",
    "            \n",
    "            # if the 'q' ker was pressed, break from the loop\n",
    "            if key==ord('q'):\n",
    "                break\n",
    "            \n",
    "print('[INFO] cleaning up ...')\n",
    "# check if we'er recording and stop recording \n",
    "if kcw.recording:\n",
    "    kcw.finish()\n",
    "\n",
    "# stop the video stream\n",
    "vs.stop if not VIDEO_PATH else vs.release()        \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending Notification From RPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c5504d6157ebd08d0b5b06f604328264fed0ac7be7e6e04c5195dc439f7520a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
