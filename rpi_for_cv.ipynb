{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(text,src):\n",
    "    cv.imshow(text,src)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackImages(scale,imgArray):\n",
    "    rows = len(imgArray)\n",
    "    cols = len(imgArray[0])\n",
    "    rowsAvailable = isinstance(imgArray[0], list)\n",
    "    width = imgArray[0][0].shape[1]\n",
    "    height = imgArray[0][0].shape[0]\n",
    "    if rowsAvailable:\n",
    "        for x in range ( 0, rows):\n",
    "            for y in range(0, cols):\n",
    "                if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]:\n",
    "                    imgArray[x][y] = cv.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x][y] = cv.resize(imgArray[x][y], (imgArray[0][0].shape[1],\n",
    "                                                                imgArray[0][0].shape[0]),None, scale, scale)\n",
    "                if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv.cvtColor( imgArray[x][y], cv.COLOR_GRAY2BGR)\n",
    "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "        hor = [imageBlank]*rows\n",
    "        hor_con = [imageBlank]*rows\n",
    "        for x in range(0, rows):\n",
    "            hor[x] = np.hstack(imgArray[x])\n",
    "        ver = np.vstack(hor)\n",
    "    else:\n",
    "        for x in range(0, rows):\n",
    "            if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                imgArray[x] = cv.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "            else:\n",
    "                imgArray[x] = cv.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale)\n",
    "            if len(imgArray[x].shape) == 2: imgArray[x] = cv.cvtColor(imgArray[x], cv.COLOR_GRAY2BGR)\n",
    "        hor= np.hstack(imgArray)\n",
    "        ver = hor\n",
    "    return ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize and Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv.imread('my_image/me.jpg')\n",
    "img_resized=cv.resize(img,(300,200))\n",
    "show_img('Fixed Resizing',img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized_aspect_ratio=imutils.resize(img,width=300)\n",
    "show_img('Aspect Ratio Resize',img_resized_aspect_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rotated_pos=imutils.rotate(img,45)\n",
    "img_rotated_neg=imutils.rotate(img,-45)\n",
    "img_rotated = stackImages(0.5,([img,img_rotated_pos],[img,img_rotated_neg]))\n",
    "\n",
    "show_img('Rotation',img_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rotated_bound=imutils.rotate_bound(img,45)\n",
    "show_img('Rotation Bound',img_rotated_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_shapes=cv.imread('chapter04-opencv_tutorial\\images\\shapes.png')\n",
    "img_copy=img_shapes.copy()\n",
    "img_gray=cv.cvtColor(img_shapes,cv.COLOR_BGR2GRAY)\n",
    "img_blur=cv.GaussianBlur(img_gray,(3,3),0)\n",
    "img_edged=cv.Canny(img_blur,50,150)\n",
    "\n",
    "\n",
    "cnts=cv.findContours(img_edged,cv.RETR_EXTERNAL,cv.CHAIN_APPROX_SIMPLE)\n",
    "# list of contours \n",
    "cnts=imutils.grab_contours(cnts)\n",
    "\n",
    "total=0\n",
    "for cnt in cnts:\n",
    "    # ignore small areas (noises)\n",
    "    if cv.contourArea(cnt) <25:\n",
    "        continue\n",
    "    cv.drawContours(img_copy,[cnt],-1,(204,0,255),2)\n",
    "    total+=1\n",
    "    \n",
    "    peri=cv.arcLength(cnt,True) #curve length \n",
    "    # print(peri)\n",
    "    approx=cv.approxPolyDP(cnt,epsilon=0.02*peri,closed=True)\n",
    "    print(len(approx)) #3=> triangle #4 =>rectangle or Square #4> => curve\n",
    "\n",
    "    obj_cor=len(approx)\n",
    "    \n",
    "    x,y,w,h=cv.boundingRect(approx)\n",
    "    \n",
    "    obj_type=''\n",
    "    if obj_cor==3:\n",
    "        obj_type='Triangle'\n",
    "    elif obj_cor==5:\n",
    "        obj_type='Pentagon'\n",
    "    elif obj_cor==6:\n",
    "        obj_type='WTFFF'\n",
    "    elif obj_cor==4:\n",
    "        asp_ratio=w/float(h)\n",
    "        if asp_ratio >0.95 and asp_ratio < 1.05 :\n",
    "            obj_type='Square'\n",
    "        else:\n",
    "            obj_type='Rectangle'\n",
    "    else:\n",
    "        obj_type='Curve'\n",
    "    \n",
    "    cv.rectangle(img_copy,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv.putText(img_copy,obj_type,\n",
    "                (x+(w//2)-20,y+(h//2)),cv.FONT_HERSHEY_COMPLEX,0.6,\n",
    "                (56, 32, 24),2)   \n",
    "    \n",
    "print(f'**** {total} objects found **** ')\n",
    "\n",
    "img_stack = stackImages(0.5,([img_shapes,img_gray,img_copy],[img_blur,img_edged,img_copy]))\n",
    "show_img('Counting Objects',img_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vars :** The method returns the __dict__ attribute for a module, class, instance, or any other object if the same has a __dict__ attribute. If the object fails to match the attribute, it raises a TypeError exception. Objects such as modules and instances have an updatable __dict__ attribute however, other objects may have written restrictions on their __dict__ attributes. vars() acts like locals() method when an empty argument is passed which implies that the locals dictionary is only useful for reads since updates to the locals dictionary are ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# # construct the argument parser and parse the arguments\n",
    "\n",
    "# ap=argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-b','--bg', required=True,help='path to the background image')\n",
    "# ap.add_argument('-f','--fg', required=True,help='path to the foreground image')\n",
    "\n",
    "# args=vars(ap.parse_args())\n",
    "\n",
    "# img_bg=cv.imread(args['bg'])\n",
    "# img_fg=cv.imread(args['fg'])\n",
    "\n",
    "# # execute this script \n",
    "\n",
    "# python [NAME].py --bg [BG_PATH] --fg [FG_PATH]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "img_bg=cv.imread(\"my_image\\me_bg.jpg\")\n",
    "img_fg=cv.imread(\"my_image\\me_fg.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show images\n",
    "# show_img('img_bg',img_bg)\n",
    "# show_img('img_fg',img_fg)\n",
    "\n",
    "# convert background and foreground images to grayscale\n",
    "img_bg_gray=cv.cvtColor(img_bg,cv.COLOR_BGR2GRAY)\n",
    "img_fg_gray=cv.cvtColor(img_fg,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# perform background subtraction | background image (int32) - foreground image (int32) |\n",
    "img_sub=img_bg_gray.astype('int32')-img_fg_gray.astype('int32')\n",
    "img_sub=np.absolute(img_sub).astype('uint8')\n",
    "\n",
    "print(f'bg : {img_bg_gray[0][:13]} ,\\nfg : {img_fg_gray[0][:13]} ,\\nsub : {img_sub[0][:13]} ')\n",
    "\n",
    "show_img('img_sub',img_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basics of Erosion:** \n",
    " \n",
    "Erodes away the boundaries of the foreground object <br>\n",
    "Used to diminish the features of an image. <br> <br>\n",
    "\n",
    "\n",
    "**Basics of dilation:**\n",
    " \n",
    "\n",
    "Increases the object area <br>\n",
    "Used to accentuate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thresh=cv.threshold(img_sub,0,255,cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n",
    "img_erode=cv.erode(img_thresh,None,iterations=1)\n",
    "img_dilate=cv.dilate(img_erode,None,iterations=1)\n",
    "\n",
    "# # show images\n",
    "# img_subtraction = stackImages(0.7,([img_bg_gray,img_fg_gray,img_sub],[img_thresh,img_erode,img_dilate]))\n",
    "# show_img('img_subtraction',img_subtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contour detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours in the thresholded difference map and then initialize our bounding box regions\n",
    "# that contains the *entire* region of motion. \n",
    "\n",
    "cnts=cv.findContours(img_thresh.copy(),cv.RETR_EXTERNAL,cv.CHAIN_APPROX_SIMPLE)\n",
    "cnts=imutils.grab_contours(cnts)\n",
    "\n",
    "(min_x,min_y)=(np.inf,np.inf)\n",
    "(max_x,max_y)=(-np.inf,-np.inf)\n",
    "\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "    # compute the bounding box of the contour\n",
    "    (x,y,w,h)=cv.boundingRect(c)\n",
    "    \n",
    "    \n",
    "    # reduse noises by enforcing  requirements on the bounding box size \n",
    "    if w>20 and h>20:\n",
    "        # update our bookkeeping variables\n",
    "        min_x=min(min_x,x)\n",
    "        min_y=min(min_y,y)\n",
    "        max_x=max(max_x,x+w-1)\n",
    "        max_y=max(max_y,y+h-1)\n",
    "        \n",
    "# draw a rectangle surrounding the region of motion\n",
    "cv.rectangle(img_fg,(min_x,min_y), (max_x,max_y),(0,255,0) ,3)\n",
    "\n",
    "# show images\n",
    "img_subtraction = stackImages(0.3,([img_bg_gray,img_fg_gray,img_sub],[img_thresh,img_dilate,img_fg]))\n",
    "show_img('img_subtraction',img_subtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Object and Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load images\n",
    "img_family=cv.imread('my_image\\\\family.jpg')\n",
    "\n",
    "# resize image\n",
    "img_family=imutils.resize(img_family,width=700)\n",
    "\n",
    "# convert image to grayscale\n",
    "img_family_gray=cv.cvtColor(img_family,cv.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**detectMultiScale :**\n",
    "\n",
    "**scaleFactor** – Parameter specifying how much the image size is reduced at each image scale.<br>\n",
    "1.05 is a good possible value for this, which means you use a small step for resizing, i.e. reduce the size by 5%, you increase the chance of a matching size with the model for detection is found.<br><br>\n",
    "\n",
    "\n",
    "**minNeighbors** – Parameter specifying how many neighbors each candidate rectangle should have to retain it. <br>\n",
    "This parameter will affect the quality of the detected faces. Higher value results in fewer detections but with higher quality. 3~6 is a good value for it.<br><br>\n",
    "\n",
    "**minSize** – Minimum possible object size. Objects smaller than that are ignored.<br>\n",
    "This parameter determines how small size you want to detect. You decide it! Usually, [30, 30] is a good start for face detection.<br><br>\n",
    "\n",
    "**maxSize** – Maximum possible object size. Objects bigger than this are ignored.\n",
    "This parameter determines how big size you want to detect. Again, you decide it! Usually, you don't need to set it manually, the default value assumes you want to detect without an upper limit on the size of the face.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load face detector \n",
    "detector=cv.CascadeClassifier('haarcascade_frontalface_alt_tree.xml')\n",
    "\n",
    "# detect faces in the image\n",
    "rects=detector.detectMultiScale(img_family_gray,scaleFactor=1.05,minNeighbors=3,minSize=(20,20),flags=cv.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "# total number of faces in image\n",
    "print(f'[INFO] detected {len(rects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the bounding boxes and draw a rectangle around each face\n",
    "for (x,y,w,h) in rects:\n",
    "\n",
    "    cv.rectangle(img_family,(x,y), (x+w,y+h),(0,255,0) ,2)\n",
    "\n",
    "show_img('img family',img_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[INFO] starting video stream ')\n",
    "# initialize the video stream\n",
    "vs = VideoStream(src=-1).start()\n",
    "# vs=VideoStream(src=0,usePiCamera=True,resolution=(640,480))\n",
    "\n",
    "# start thewebcam video stream and turn off the autofoucos setting\n",
    "vs.stream.set(cv.CAP_PROP_AUTOFOCUS,0)\n",
    "\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    frame = imutils.resize(frame,width=400)\n",
    "    \n",
    "    # show the output frame\n",
    "    cv.imshow('Frame',frame)\n",
    "    key= cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the 'q' ker was pressed, break from the loop\n",
    "    if key== ord('q'):\n",
    "        break\n",
    "\n",
    "# do a bit for cleanup\n",
    "cv.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_autofocus(vs,autofocus=True):\n",
    "    # set the autofocus camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_AUTOFOCUS,1 if autofocus else 0)\n",
    "    print(f'[INFO] autofocus has been set to {\"ON\" if autofocus else \"OFF\"}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualAutofocus=vs.stream.get(cv.CAP_PROP_AUTOFOCUS)\n",
    "    print(f'[INFO] actual autofocus {actualAutofocus}')\n",
    "\n",
    "\n",
    "def toggle_auto_whitebalance(vs,autowb=True):\n",
    "    # set the auto whitebalance camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_AUTO_WB,1 if autowb else 0)\n",
    "    print(f'[INFO] auto white balance has been set to {\"ON\" if autowb else \"OFF\"}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualAutoWB=vs.stream.get(cv.CAP_PROP_AUTO_WB)\n",
    "    print(f'[INFO] actual auto white balance {actualAutoWB}')\n",
    "\n",
    "def set_zoom(vs,zoom=100):\n",
    "    # set the zoom camera property  ON or OFF\n",
    "    vs.stream.set(cv.CAP_PROP_ZOOM,zoom)\n",
    "    print(f'[INFO] zoom has been set to {zoom}')\n",
    "\n",
    "    # read back the property to ensure it was set\n",
    "    actualZoom=vs.stream.get(cv.CAP_PROP_ZOOM)\n",
    "    print(f'[INFO] actual zoom {actualZoom}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[INFO] starting video stream ')\n",
    "# initialize the video stream\n",
    "url='http://192.168.0.190:8080/video'\n",
    "vs = VideoStream(src=url).start()\n",
    "\n",
    "# initialize the camera parameter settings\n",
    "autofocus=True\n",
    "autowb=True\n",
    "zoom=100\n",
    "# vs.stream\n",
    "# vs.set(cv.CAP_PROP_ZOOM,50)\n",
    "# time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    frame = imutils.resize(frame,width=600)\n",
    "    \n",
    "    # show the output frame\n",
    "    cv.imshow('Frame',frame)\n",
    "    key= cv.waitKey(1)\n",
    "    \n",
    "    # handle *q* keypresses for \"quit\"\n",
    "\n",
    "    if key== ord('q'):\n",
    "        break  \n",
    "\n",
    "    # handle *f* keypresses for \"autofocus\"\n",
    "    elif key== ord('f'):\n",
    "        # toggle autofocus and set the camera property\n",
    "        autofocus= not autofocus\n",
    "        toggle_autofocus(vs,autofocus)\n",
    "\n",
    "    # handle *w* keypresses for \"auto white balance\"\n",
    "    elif key== ord('w'):\n",
    "        # toggle auto white balance and set the camera property\n",
    "        autowb= not autowb\n",
    "        toggle_auto_whitebalance(vs,autowb)\n",
    "\n",
    "    # handle *i* keypresses for \"zoom in\"\n",
    "    elif key== ord('i'):\n",
    "        # increase zoom  and set the camera property\n",
    "        zoom+=1\n",
    "        set_zoom(vs,zoom)\n",
    "\n",
    "    # handle *o* keypresses for \"zoom out\"\n",
    "    elif key== ord('o'):\n",
    "        # decrease zoom  and set the camera property\n",
    "        zoom-=1\n",
    "        set_zoom(vs,zoom)\n",
    "\n",
    "# reset camera parameter settings\n",
    "toggle_autofocus(vs,True)\n",
    "toggle_auto_whitebalance(vs,True)\n",
    "set_zoom(vs,100)\n",
    "\n",
    "# do a bit for cleanup\n",
    "vs.stop()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Lapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Capture Time Lapse Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import signal\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_handler(sig,frame):\n",
    "    print('[INFO] You pressed `ctrl + c`! Your pictures are saved' \\\n",
    "          ' in the output directory you specified :) ')\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "# ap=argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-o','--output',required=True,\n",
    "#     help='Path to the output directory')\n",
    "# ap.add_argument('-d','--delay',type=float,default=5.0,\n",
    "#     help='Delay in seconds between frames captured')\n",
    "# ap.add_argument('-dp','--display',type=int,default=0,\n",
    "#     help='Boolean used to indicate if frames should be displayed')\n",
    "\n",
    "# arg=vars(ap.parse_args())\n",
    "\n",
    "OUTPUT_PATH='my_image/output_dir'\n",
    "DELAY=2\n",
    "DISPLAY=True\n",
    "\n",
    "\n",
    "# initialize the output directory path and create the output directory \n",
    "output_dir=os.path.join(OUTPUT_PATH,\n",
    "        datetime.now().strftime('%Y-%m-%d-%H%M'))\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "# initialize the video stream \n",
    "print('[INFO] warming up camera ...')\n",
    "\n",
    "url='http://192.168.0.116:8080/video'\n",
    "vs=VideoStream(src=url,resolution=(1920,1280), framerate=30).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# set the frame count to zero \n",
    "count=0\n",
    "\n",
    "# signal trap to handle keyboard interrupt \n",
    "signal.signal(signal.SIGINT,signal_handler)\n",
    "print('[INFO] press `ctrl + c` to exit, or \"q\" to quit if you have the display option on ...')\n",
    "\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream \n",
    "while True:\n",
    "    # grab the frame from the video stream and resize it\n",
    "    frame= vs.read()\n",
    "    \n",
    "    # draw the timestamp on the frame\n",
    "    ts=datetime.now().strftime('%A %d %B %Y %I:%M:%S%p')\n",
    "    cv.putText(frame,ts,(10,frame.shape[0]-10),cv.FONT_HERSHEY_SIMPLEX,0.35,(0,0,255),1)\n",
    "\n",
    "    # write the current frame to output directory\n",
    "    filename=f'{str(count).zfill(16)}.jpg' \n",
    "    cv.imwrite(os.path.join(output_dir,filename),frame)\n",
    "\n",
    "    # display the frame and detect keypressess if the flag is set\n",
    "    if DISPLAY:\n",
    "        # show the output frame\n",
    "        cv.imshow('Frame',frame)\n",
    "        key= cv.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the 'q' ker was pressed, break from the loop\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # increment the count\n",
    "        count+=1\n",
    "\n",
    "        # sleep for specified number of seconds\n",
    "        time.sleep(DELAY)\n",
    "\n",
    "# do a bit for cleanup\n",
    "print('[INFO] cleaning up ...')\n",
    "if DISPLAY:\n",
    "    cv.destroyAllWindows()\n",
    "vs.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing Time Lapse Images into a Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import paths\n",
    "import progressbar\n",
    "import signal\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('my_image/output_dir/2022-05-07-1256/0000000000000000.jpg'.split(os.path.sep)[-1][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH='my_image/output_dir/2022-05-07-1256/'\n",
    "OUTPUT_PATH='my_image/video_dir'\n",
    "# Frames per secend\n",
    "FPS=15\n",
    "\n",
    "# function to get the frame number from the image path\n",
    "def get_number(img_paths):\n",
    "    return int(img_paths.split(os.path.sep)[-1][:-4])\n",
    "\n",
    "\n",
    "\n",
    "# initialize the FourCC and video writer\n",
    "fourcc=cv.VideoWriter_fourcc(*'MJPG')\n",
    "writer=None\n",
    "\n",
    "# grab the paths to the images, and initialize output file name and output path\n",
    "img_paths=list(paths.list_images(INPUT_PATH))\n",
    "output_file=f'{INPUT_PATH.split(os.path.sep)[2]}.avi'\n",
    "output_path=os.path.join(OUTPUT_PATH,output_file)\n",
    "\n",
    "print(f'[INFO] building {output_path} ...')\n",
    "\n",
    "\n",
    "# initialize the progress bar\n",
    "widgets=['Building Video : ',progressbar.Percentage(),' ',progressbar.Bar(),' ', progressbar.ETA()]\n",
    "pbar=progressbar.ProgressBar(maxval=len(img_paths),widgets=widgets).start()\n",
    "\n",
    "# loop over all stored image paths\n",
    "for (i,img_path) in enumerate(sorted(img_paths,key=get_number)):\n",
    "    # print('helooo')\n",
    "    # load the image\n",
    "    img=cv.imread(img_path)\n",
    "\n",
    "    # initialize the video writer if needed\n",
    "    if writer is None:\n",
    "        (H,W) = img.shape[:2]\n",
    "        writer=cv.VideoWriter(output_path,fourcc,FPS,(W,H),True)\n",
    "    \n",
    "    # write the image to output video\n",
    "    writer.write(img)\n",
    "    pbar.update(i)\n",
    "\n",
    "\n",
    "# release the writer object \n",
    "print('[INFO] cleaning up ...')\n",
    "pbar.finish()\n",
    "writer.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bird Feeder Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install json_minify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### JSON-minify : \n",
    "`JSON-minify` minifies blocks of JSON-like content into valid JSON by removing\n",
    "all whitespace *and* JS-style comments (single-line `//` and multi-line\n",
    "`/* .. */`).\n",
    "\n",
    "With `JSON-minify`, you can maintain developer-friendly JSON documents, but\n",
    "minify them before parsing or transmitting them over-the-wire.\n",
    "\n",
    "###### json.loads() : \n",
    "json. loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confPath='Resources/src/bird feeder monitor/config/gmg.json'\n",
    "json.loads(json_minify(open(confPath).read()))\n",
    "# json_minify(open(confPath).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.config import Conf\n",
    "from Resources.src.utils.clipwriter import KeyClipWriter\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import datetime \n",
    "import imutils\n",
    "import time \n",
    "import sys\n",
    "import cv2 as cv\n",
    "import os\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the JSON configuration file \n",
    "CONF_PATH='Resources/src/bird_feeder_monitor/config/gmg.json'\n",
    "# path to the optional input video file \n",
    "VIDEO_PATH='Resources/src/bird_feeder_monitor/birds_10min.mp4'\n",
    "# VIDEO_PATH=''\n",
    "\n",
    "# STREAM_URL='http://192.168.0.190:8080/video'\n",
    "STREAM_URL=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load our configuration settings\n",
    "conf=Conf(CONF_PATH)\n",
    "# conf.__getitem__('fps')\n",
    "\n",
    "# check if we are using a camera and start video stream \n",
    "if not VIDEO_PATH:\n",
    "    vs=VideoStream(src=STREAM_URL,resolution=(1920,1280), framerate=30,usePiCamera=conf['picamera']).start()\n",
    "    time.sleep(3.0)\n",
    "else:\n",
    "    print(f'[INFO] opening video file {VIDEO_PATH}')\n",
    "    vs=cv.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# OpenCV background subtraction\n",
    "OPENCV_BG_SUBTRACTIONS={\n",
    "    'CNT': cv.bgsegm.createBackgroundSubtractorCNT(),\n",
    "    'GMG': cv.bgsegm.createBackgroundSubtractorGMG(),\n",
    "    'MOG': cv.bgsegm.createBackgroundSubtractorMOG(),\n",
    "    'GSOC': cv.bgsegm.createBackgroundSubtractorGSOC(),\n",
    "    'LSBP': cv.bgsegm.createBackgroundSubtractorLSBP()\n",
    "}\n",
    "\n",
    "# create our background substractor\n",
    "fgbg=OPENCV_BG_SUBTRACTIONS[conf['bg_sub']]\n",
    "\n",
    "# create erosion and dilation kernels\n",
    "eKernel=np.ones(tuple(conf['erode']['kernel']),'uint8')\n",
    "dKernel=np.ones(tuple(conf['dilate']['kernel']),'uint8')\n",
    "\n",
    "# initialize key clip writer, the consecutive number of frames without \n",
    "# motion and frame since the last snapshot was written\n",
    "kcw=KeyClipWriter(bufSize=conf['keyclipwriter_buffersize']) \n",
    "frames_without_motion=0\n",
    "frames_since_snap=0\n",
    "\n",
    "# begin captureing \"ctrl+c\" signals\n",
    "# signal.signal(signal.SIGINT,signal_handler)\n",
    "images=' and images ...' if conf['write_snaps'] else '...'\n",
    "print(f'[INFO] detecting motion and storing videos{images}') \n",
    "\n",
    "# loop over the frames \n",
    "while True:\n",
    "    # grab a frame from the video stream\n",
    "    full_frame=vs.read()\n",
    "    \n",
    "    # if no frames was read, the stream has ended\n",
    "    if full_frame is None:\n",
    "        break\n",
    "    \n",
    "    # handle the frame whether the frame was read from a VideoCapture\n",
    "    # or VideoStream\n",
    "    full_frame=full_frame[1] if VIDEO_PATH else full_frame\n",
    "    \n",
    "    # increment number of frames since last snapshot was written\n",
    "    frames_since_snap+=1\n",
    "\n",
    "    # resize the frame apply the background subtractor to generate motion mask\n",
    "    frame=imutils.resize(full_frame,width=500)\n",
    "    mask=fgbg.apply(frame)\n",
    "    \n",
    "    # perform erosions and dilations to eliminate noise and fill gaps\n",
    "    mask=cv.erode(mask,eKernel,iterations=conf['erode']['iterations'])\n",
    "    mask=cv.dilate(mask,dKernel,iterations=conf['dilate']['iterations'])\n",
    "\n",
    "    # find contours in the mask and reset the motion status\n",
    "    cnts=cv.findContours(mask.copy(),cv.RETR_EXTERNAL,\n",
    "                         cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnts=imutils.grab_contours(cnts)\n",
    "    motion_this_frame=False\n",
    "    \n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # It is a circle which completely covers the object with minimum area\n",
    "        ((x,y),radius)=cv.minEnclosingCircle(c)\n",
    "        (rx,ry,rw,rh)=cv.boundingRect(c)\n",
    "        \n",
    "        # convert floating point values to integers\n",
    "        (x,y,radius)=[int(v) for v in (x,y,radius)]        \n",
    "        \n",
    "        # only process motion contours above the specified size\n",
    "        if radius < conf['min_radius']:\n",
    "            continue\n",
    "        # grab the current timestamp \n",
    "        timestamp=datetime.datetime.now()\n",
    "        timestring=timestamp.strftime('%Y%m%d-%H%M%S')\n",
    "        \n",
    "        # set our motion flag to indicate we have found motion and\n",
    "        # reset the motion counter\n",
    "        motion_this_frame=True\n",
    "        frames_without_motion=0\n",
    "        \n",
    "        # check if we need to annotate the frame for display\n",
    "        if conf['annotate']:\n",
    "            cv.circle(frame,(x,y),radius,(0,0,255),2)\n",
    "            cv.rectangle(frame,(rx,ry),(rx+rw,ry+rh),(0,255,0),2)\n",
    "        \n",
    "        # frame to disk\n",
    "        write_frame=frames_since_snap >= conf['frames_between_snaps']\n",
    "        \n",
    "        # check to see if should write the frame to disk\n",
    "        if conf['write_snaps'] and write_frame:\n",
    "            # construct the path to output photo and save it\n",
    "            snap_path=os.path.sep.join([conf['output_path'],timestring])\n",
    "            cv.imwrite(snap_path+'.jpg',full_frame)\n",
    "            \n",
    "            # reset the counter between snapshots\n",
    "            frames_since_snap=0\n",
    "        \n",
    "        # start recording if we aren't already\n",
    "        if not kcw.recording:\n",
    "            # construct the path to the video file\n",
    "            video_path=os.path.sep.join([conf['output_path'],timestring])\n",
    "\n",
    "            # instantiate the video codec object and start the key clip writer\n",
    "            fourcc=cv.VideoWriter_fourcc(*conf['codec'])\n",
    "            kcw.start('{}.avi'.format(video_path),fourcc,conf['fps']) \n",
    "        \n",
    "        # check if no motion was detected in this frame and then increment \n",
    "        # the number of consecutive frames without motion\n",
    "        if not motion_this_frame:\n",
    "            frames_without_motion+=1\n",
    "            \n",
    "        # update the key clip buffer\n",
    "        kcw.update(frame)\n",
    "    \n",
    "        # check to see if the number of frames without motion is above our defined threshold\n",
    "        no_motion=frames_without_motion >= conf['keyclipwriter_buffersize']\n",
    "        \n",
    "        # stop recording if there is no motion\n",
    "        if kcw.recording and no_motion:\n",
    "            kcw.finish()\n",
    "        \n",
    "        # check to see if we'er displaying the frame to our screen\n",
    "        if conf['display']:\n",
    "            # display the frame and grab keypresses\n",
    "            cv.imshow('Frames',frame) \n",
    "            key=cv.waitKey(1) & 0xFF\n",
    "            \n",
    "            # if the 'q' ker was pressed, break from the loop\n",
    "            if key==ord('q'):\n",
    "                break\n",
    "            \n",
    "print('[INFO] cleaning up ...')\n",
    "# check if we'er recording and stop recording \n",
    "if kcw.recording:\n",
    "    kcw.finish()\n",
    "\n",
    "# stop the video stream\n",
    "vs.stop if not VIDEO_PATH else vs.release()        \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending Notification From RPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.notifications import TelegramSender\n",
    "from Resources.src.utils.config import Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_PATH='Resources\\src\\sending_notifications\\config\\config.json'\n",
    "conf=Conf(CONF_PATH)\n",
    "sender=TelegramSender(conf)\n",
    "\n",
    "# send a text message\n",
    "print('[INFO] sending txt message ...')\n",
    "sender.send_msg('Hello Mr S-W-G-D, Welcome To Computer Vision World...')\n",
    "print('[INFO] txt message sent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting Mail Delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.notifications import TelegramSender\n",
    "from Resources.src.utils.config import Conf\n",
    "from imutils.video import VideoStream\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONF_PATH='Resources\\src\\sending_notifications\\config\\config.json'\n",
    "STREAM_URL='http://192.168.0.190:8080/video'\n",
    "\n",
    "conf=Conf(CONF_PATH)\n",
    "sender=TelegramSender(conf)\n",
    "\n",
    "# initialize the flags for mailbox open and notfication sent\n",
    "mailbox_open=False\n",
    "msg_sent=False\n",
    "\n",
    "print('[INFO] warming up camera ...')\n",
    "\n",
    "vs=VideoStream(STREAM_URL).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    frame=vs.read()\n",
    "    frame=imutils.resize(frame,width=200)\n",
    "    # 0 => darker , 255 => brighter\n",
    "    gray=cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # set the previous mailbox status\n",
    "    mailbox_pre_open=mailbox_open\n",
    "\n",
    "    # calculate the average of all pixels where a higher mean\n",
    "    # indicating that there is more light coming into the mailbox,\n",
    "    # the determine if the mailbox is currently open\n",
    "    mean=np.mean(gray)\n",
    "    mailbox_open=mean > conf['thresh']\n",
    "    \n",
    "    # if the mailbox is open and previously it was closed, it means\n",
    "    # the mailbox has been just opened\n",
    "    if mailbox_open and not mailbox_pre_open:\n",
    "        # record the start time\n",
    "        start_time=datetime.now()\n",
    "    \n",
    "    # if the mailbox is open then there are 2 possibilities,\n",
    "    # 1) it's left open for more than  the *threshold* seconds\n",
    "    # 2) it's closed in less than or equal to the *threshold* seconds\n",
    "    elif mailbox_pre_open:\n",
    "        elapsed_time=(datetime.now()-start_time).seconds\n",
    "        mailbox_left_open=elapsed_time>conf['open_threshold_seconds']\n",
    "        \n",
    "        # handel when the mailbox was left open\n",
    "        if mailbox_open and mailbox_left_open:\n",
    "            #if a msg has not been sent yet, then send a msg\n",
    "            if not msg_sent:\n",
    "                msg=f\"Your mailbox at {conf['address_id']} has been left open for longer\"\\\n",
    "                f\"than {conf['open_threshold_seconds']} seconds. It is possible that you or\"\\\n",
    "                \"the mailman didn't close your mailbox.\"\n",
    "                sender.send_photo(msg=msg,img_file=frame)\n",
    "                msg_sent=True\n",
    "            \n",
    "        # check to see if the mailbox has been closed\n",
    "        elif not mailbox_open:\n",
    "            # if a msg has already been sent, then just set \n",
    "            # the our boolean to False for the next iteration\n",
    "            \n",
    "            if msg_sent:\n",
    "                msg_sent:False\n",
    "            # if msg has not been sent, then send a msg\n",
    "            else :\n",
    "                # record the end time and calculate the total time in seconds\n",
    "                end_time=datetime.now()\n",
    "                total_seconds=(end_time-start_time).seconds\n",
    "                date_opened=date.today().strftime(\"%A, %B %d %Y\")\n",
    "                \n",
    "                # build the msg and send\n",
    "                msg=f\"Your mailbox at {conf['address_id']} was opened on {date_opened}\"\\\n",
    "                f\"at for {total_seconds} seconds\"  \n",
    "                sender.send_photo(msg=msg,img_file=frame)\n",
    "                \n",
    "    if conf['display']:  \n",
    "        cv.imshow('frame',frame)\n",
    "        key=cv.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key==ord('q'):\n",
    "            break\n",
    "\n",
    "# clean up\n",
    "cv.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Surveillance and Web Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.motion_detection import SingleMotionDetector\n",
    "from imutils.video import VideoStream\n",
    "from flask import Response \n",
    "from flask import Flask \n",
    "from flask import render_template \n",
    "import datetime \n",
    "import threading \n",
    "import argparse \n",
    "import imutils\n",
    "import time \n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "\n",
    "# ap=argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-b','--bg', required=True,help='path to the background image')\n",
    "# ap.add_argument('-f','--fg', required=True,help='path to the foreground image')\n",
    "\n",
    "# args=vars(ap.parse_args())\n",
    "\n",
    "# img_bg=cv.imread(args['bg'])\n",
    "# img_fg=cv.imread(args['fg'])\n",
    "\n",
    "# # execute this script \n",
    "\n",
    "# python [NAME].py --bg [BG_PATH] --fg [FG_PATH]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_COUNT=32\n",
    "URL='http://192.168.0.190:8080/video'\n",
    "IP='127.0.0.1'\n",
    "PORT=8080\n",
    "\n",
    "# initialize the output frame and a lock used to ensure thread-safe\n",
    "# exchange of the output frames (useful for multiple browsers/tabs are viewing the stream)\n",
    "output_frame=None\n",
    "lock=threading.Lock()\n",
    "\n",
    "# initialize a flask object\n",
    "app=Flask(__name__,template_folder='Resources\\\\src\\\\video_surveillance_and_web_streaming')\n",
    "\n",
    "print('[INFO] starting video stream ')\n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "# vs = VideoStream(src=0).start()\n",
    "vs=VideoStream(src=URL,resolution=(1920,1280), framerate=32).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    # return the rendered template\n",
    "    return render_template('index.html')\n",
    "\n",
    "'''\n",
    "frame_count : is the min number of required frames to build our background bg in the SingleMotionDetector class\n",
    "'''\n",
    "def detect_motion(frame_count):\n",
    "    # grab global references to the video stream, output frame, and lock variables\n",
    "    global vs,output_frame,lock\n",
    "    \n",
    "    # initialize the motion detector and the total number of frames read thus far\n",
    "    md=SingleMotionDetector(accum_weight=0.1)\n",
    "    total=0\n",
    "    \n",
    "    # loop over the frames from the video stream \n",
    "    while True:\n",
    "        # grab the frame from the video stream and resize it\n",
    "        # convert the frame to grayscale and blur it\n",
    "        frame= vs.read()\n",
    "        frame = imutils.resize(frame,width=800)\n",
    "        gray=cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "        gray=cv.GaussianBlur(gray,(7,7),0)\n",
    "        \n",
    "        # draw the timestamp on the frame\n",
    "        ts=datetime.datetime.now().strftime('%A %d %B %Y %I:%M:%S%p')\n",
    "        cv.putText(frame,ts,(10,frame.shape[0]-10),cv.FONT_HERSHEY_SIMPLEX,0.35,(0,0,255),1)\n",
    "        \n",
    "        # if the total number of frames has reached a sufficient\n",
    "        # number to construct a reasonable background model, then\n",
    "        # continue to process the frame\n",
    "        if total>frame_count:\n",
    "            # detect motion in the image\n",
    "            motion=md.detect(gray)\n",
    "        \n",
    "            # check to see if motion was found in the frame\n",
    "            if motion is not None:\n",
    "                # unpack the tuple and draw the box surrounding the \"motion area\" on the output frame\n",
    "                (thresh,(min_x,min_y,max_x,max_y))=motion\n",
    "                cv.rectangle(frame,(min_x,min_y),(max_x,max_y),(0,0,255),2)\n",
    "        \n",
    "        # update the background model and increment the total number of frames read thus far\n",
    "        md.update(gray)\n",
    "        total+=1\n",
    "        \n",
    "        # acquire the lock,set the output frame, and release the lock\n",
    "        with lock:\n",
    "            output_frame=frame.copy()\n",
    "\n",
    "'''\n",
    "this func is a Python generator used to encode our output_frame as JPEG data \n",
    "'''\n",
    "def generate():\n",
    "    # grab global references to the output frame, and lock variables\n",
    "    global output_frame,lock\n",
    "    \n",
    "    # loop over the frames from the output stream \n",
    "    while True:\n",
    "        # wait until the lock is acquired\n",
    "        with lock:\n",
    "            # check if the output frame is available, otherwise skip \n",
    "            # the iteration of the loop\n",
    "            if output_frame is None:\n",
    "                continue\n",
    "            \n",
    "            # encode the frame was successfully encoded\n",
    "            (flag,encoded_img)=cv.imencode('.jpg',output_frame)\n",
    "            \n",
    "            # ensure the frame was successfully encoded\n",
    "            if not flag:\n",
    "                continue\n",
    "            \n",
    "            # yield the output frame in the bytecode format\n",
    "            yield(b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n'+bytearray(encoded_img)+b'\\r\\n')\n",
    "             \n",
    "\n",
    "'''\n",
    "output of this func is the live motion detection output,encoded as a byte array via the generate function.\n",
    "'''        \n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    # return the response generated along with the specific media type (mime type)\n",
    "    return Response(generate(),mimetype='multipart/x-mixed-replace;boundary=frame')       \n",
    "        \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # start a thread that will perform motion detection\n",
    "    t=threading.Thread(target=detect_motion,args=(FRAME_COUNT,))\n",
    "    t.daemon=True\n",
    "    t.start()\n",
    "    \n",
    "    # start the flask app\n",
    "    app.run(host=IP,port=PORT,debug=True,threaded=True,use_reloader=False)\n",
    "    \n",
    "# release the video stream pointer\n",
    "vs.stop()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Multiple Cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SIFT Feature Extraction in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from imutils.video import VideoStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "sift=cv.SIFT_create()\n",
    "\n",
    "# feature matching \n",
    "bf=cv.BFMatcher(cv.NORM_L2,crossCheck=True)\n",
    "\n",
    "img_1=cv.imread('Resources\\images\\me.jpg')\n",
    "img_2=cv.imread('Resources\\images\\me.jpg')\n",
    "\n",
    "keypoint_1,descriptor_1=sift.detectAndCompute(img_1,None)\n",
    "keypoint_2,descriptor_2=sift.detectAndCompute(img_2,None)\n",
    "\n",
    "matches=bf.match(descriptor_1,descriptor_2)\n",
    "matches=sorted(matches,key=lambda x:x.distance)\n",
    "\n",
    "img_3=cv.drawMatches(img_1,keypoint_1,img_2,keypoint_2,matches[:50],img_2,flags=2)\n",
    "\n",
    "cv.imshow('SIFT',img_3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "sift=cv.SIFT_create()\n",
    "\n",
    "# feature matching \n",
    "bf=cv.BFMatcher(cv.NORM_L2,crossCheck=True)\n",
    "\n",
    "URL='http://192.168.0.190:8080/video'\n",
    "cap=cv.VideoCapture(0)\n",
    "vs=VideoStream(src=URL,resolution=(1920,1280), framerate=32).start()\n",
    "time.sleep(2.0)\n",
    "while cap.read():\n",
    "    # read images\n",
    "    src,img1=cap.read()\n",
    "    img2= vs.read()\n",
    "    start=time.time()\n",
    "\n",
    "    img1=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)\n",
    "    img2=cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    "    keypoint_1,descriptor_1=sift.detectAndCompute(img1,None)\n",
    "    keypoint_2,descriptor_2=sift.detectAndCompute(img2,None)\n",
    "\n",
    "    matches=bf.match(descriptor_1,descriptor_2)\n",
    "    matches=sorted(matches,key=lambda x:x.distance)\n",
    "    \n",
    "    end=time.time()\n",
    "    totalTime=end-start\n",
    "\n",
    "    fps=1/totalTime\n",
    "    \n",
    "    img3=cv.drawMatches(img1,keypoint_1,img2,keypoint_2,matches[:100],img_2,flags=2)\n",
    "    cv.putText(img3, f'FPS: {int(fps)}', (20,450), cv.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    cv.imshow('SIFT', img3)\n",
    "\n",
    "    if cv.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resources.src.utils.using_multiple_cameras.panorma import Stitcher\n",
    "from Resources.src.utils.using_multiple_cameras.pedestriandetector import PedestrianDetector\n",
    "from imutils.video import VideoStream\n",
    "import imutils\n",
    "import time \n",
    "import datetime \n",
    "import cv2 as cv\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASCADE_PATH='Resources\\\\src\\\\using_multiple_cameras\\\\cascade\\\\haarcascade_fullbody.xml'\n",
    "RIGHT_URL='http://192.168.0.190:8080/video'\n",
    "LEFT_URL='http://192.168.0.185:8080/video'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting cameras ... \n",
      "[INFO] cleaning up ...\n"
     ]
    }
   ],
   "source": [
    "# initialize the image stitcher and create a pedestrian detector object\n",
    "stitcher=Stitcher()\n",
    "pd=PedestrianDetector(CASCADE_PATH)\n",
    "\n",
    "\n",
    "print('[INFO] starting cameras ... ')\n",
    "# left_vs = VideoStream(src=0).start()\n",
    "left_vs =VideoStream(src=LEFT_URL,resolution=(1920,1280), framerate=32).start()\n",
    "right_vs =VideoStream(src=RIGHT_URL,resolution=(1920,1280), framerate=32).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "\n",
    "# loop over the frames from the video streams\n",
    "while True:\n",
    "    # grab the frames from the respective video stream\n",
    "    left_frame= left_vs.read()\n",
    "    right_frame= right_vs.read()\n",
    "    \n",
    "    left_frame = imutils.resize(left_frame,width=500)\n",
    "    right_frame = imutils.resize(right_frame,width=500)\n",
    "    \n",
    "    # switch the frames together to from the panorama\n",
    "    # IMPORTANT : you might have to change this line of code \n",
    "    # depending on how your cameras are oriented; frames\n",
    "    # should be supplied in left-to-right order\n",
    "    result_frame=stitcher.stitch([left_frame,right_frame])\n",
    "    \n",
    "    # no homography could be computed\n",
    "    if result_frame is None:\n",
    "        print('[INFO] homography could not be computed')\n",
    "        break\n",
    "    \n",
    "    # convert the stitched frame to grayscale and find pedestrians in the image\n",
    "    gray=cv.cvtColor(result_frame,cv.COLOR_BGR2GRAY)\n",
    "    pedestrianRects=pd.detect(gray,scaleFactor=1.1,\n",
    "                              minNeighbors=5,\n",
    "                              minSize=(30,30))\n",
    "    \n",
    "    # loop over the pedestrian and draw a rectangle around each \n",
    "    for(x,y,w,h) in pedestrianRects:\n",
    "        cv.rectangle(result_frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    \n",
    "    # draw the timestamp on the image\n",
    "    timestamp=datetime.datetime.now()\n",
    "    ts=timestamp.strftime('%A %d %B %Y %I:%M:%S%p')\n",
    "    cv.putText(result_frame,ts,(10,result_frame.shape[0]-10),\n",
    "               cv.FONT_HERSHEY_SIMPLEX,0.35,(0,0,255),1)\n",
    "    \n",
    "    \n",
    "    # show the output frame\n",
    "    cv.imshow('Result',result_frame)\n",
    "    key= cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    # handle *q* keypresses for \"quit\"\n",
    "\n",
    "    if key== ord('q'):\n",
    "        break  \n",
    "\n",
    "# do a bit for cleanup\n",
    "print('[INFO] cleaning up ...')\n",
    "cv.destroyAllWindows()\n",
    "left_vs.stop()\n",
    "right_vs.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cca4a18291555fa8b75c5e05ff9c4301db9ff4700140e0fa67594393bf87e035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
